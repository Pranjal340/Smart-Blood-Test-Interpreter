{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNivx1mKWAOyWjWjS4QsMAE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cpPoVXyOB5z6","executionInfo":{"status":"ok","timestamp":1744126342875,"user_tz":-330,"elapsed":4439,"user":{"displayName":"Pranjal Agrawal","userId":"10165235319896773094"}},"outputId":"1640ea34-61e7-4791-cf55-43f3187c69b3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n","Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n","Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"]}],"source":["!pip install pandas openpyxl"]},{"cell_type":"markdown","source":["**Step 1**"],"metadata":{"id":"-eBXy4ZaCBOx"}},{"cell_type":"code","source":["# Convert all the date values in the mentioned column to \"-\"\n","\n","import pandas as pd\n","import re\n","import datetime\n","\n","# Load the Excel file\n","file_path = 'Feb.csv'\n","output_file = 'cleaned_data_1.xlsx'\n","\n","# Read the Excel file\n","df = pd.read_csv(file_path)\n","\n","# Define a pattern to match date formats like '1900-01-01 01:40:00' or '2025-10-05 00:00:00'\n","date_pattern = re.compile(r'^\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}$')\n","\n","# Function to check if a value is a date string\n","def is_date(value):\n","    if not isinstance(value, str):\n","        return False\n","    return bool(date_pattern.match(value))\n","\n","# Function to process each cell\n","def process_cell(value):\n","    if pd.isna(value):\n","        return value\n","\n","    # Convert to string for checking\n","    str_value = str(value)\n","\n","    # Check if it's a date format\n","    if is_date(str_value):\n","        return '-'\n","\n","    # Return original value if not a date\n","    return value\n","\n","# Apply the function to the problematic column\n","column_name = \"LR.RESULT_VALUE||(CASEWHENDBMS_LOB.GETLENGTH(LRP.RESULTVALUE)>3000THEN'/***********DATATRUNCATED***********/'||TO_CHAR(DBMS_LOB.SUBSTR(LRP.RESULTVALUE,3000,1))||'/***********DATATRUNCATED***********/'ELSETO_CHAR(LRP.RESULTVALUE)END)||(CASEWHENLR.RESUL\"\n","\n","# Process all columns in case the column name is different or there are other columns with dates\n","for column in df.columns:\n","    df[column] = df[column].apply(process_cell)\n","\n","# Save the processed dataframe to a new Excel file\n","df.to_excel(output_file, index=False)\n","\n","print(f\"Processed file saved as {output_file}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"46w99_oZB8Ix","executionInfo":{"status":"ok","timestamp":1744128520728,"user_tz":-330,"elapsed":188267,"user":{"displayName":"Pranjal Agrawal","userId":"10165235319896773094"}},"outputId":"91f84275-2443-43d2-b238-331e4d0d81c9"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-4-f091a1f2e3a8>:12: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n","  df = pd.read_csv(file_path)\n"]},{"output_type":"stream","name":"stdout","text":["Processed file saved as cleaned_data_1.xlsx\n"]}]},{"cell_type":"markdown","source":["**Step 2**"],"metadata":{"id":"kOQuNVVbGNCt"}},{"cell_type":"code","source":["# Remove all the rows which have any alphabets, '/', '-', '+', '<', '>', or ':' in the mentioned column\n","\n","import pandas as pd\n","\n","# Load the Excel file with the second row as header\n","file_path = 'cleaned_data_1.xlsx'  # Replace with your actual file path\n","df = pd.read_excel(file_path)  # Set header=1 to use the second row as column names\n","\n","# Specify the column to clean\n","column_to_clean = \"LR.RESULT_VALUE||(CASEWHENDBMS_LOB.GETLENGTH(LRP.RESULTVALUE)>3000THEN'/***********DATATRUNCATED***********/'||TO_CHAR(DBMS_LOB.SUBSTR(LRP.RESULTVALUE,3000,1))||'/***********DATATRUNCATED***********/'ELSETO_CHAR(LRP.RESULTVALUE)END)||(CASEWHENLR.RESUL\"\n","\n","# Step 1: Remove rows where the column contains blank spaces or any alphabetic characters\n","df = df[~df[column_to_clean].str.contains(r'[a-zA-Z\\s]', na=False)]\n","\n","# Step 2: Remove rows where the specified column is empty or NaN\n","df = df[df[column_to_clean].notna() & (df[column_to_clean].str.strip() != '')]\n","\n","# Step 3: Remove rows where the column contains '/', '-', '+', '<', '>', or ':'\n","df = df[~df[column_to_clean].str.contains(r'[\\/\\-\\+<>:]', na=False)]\n","\n","# Save the cleaned data to a new Excel file\n","output_file_path = 'cleaned_data_final.xlsx'\n","df.to_excel(output_file_path, index=False)\n","\n","print(f\"Cleaned data saved to {output_file_path}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WMfPIDUSC3S2","executionInfo":{"status":"ok","timestamp":1744129017206,"user_tz":-330,"elapsed":231020,"user":{"displayName":"Pranjal Agrawal","userId":"10165235319896773094"}},"outputId":"cd08882f-ea74-498f-c660-2db1edaf43c0"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Cleaned data saved to cleaned_data_final.xlsx\n"]}]},{"cell_type":"markdown","source":["**Step 3**"],"metadata":{"id":"IfY49NURGQPZ"}},{"cell_type":"code","source":["# Make it such that onr unique id is in one row only and rest of the thing\n","\n","import pandas as pd\n","\n","# Read the Excel file with headers from the second row\n","df = pd.read_excel('cleaned_data_final.xlsx')\n","\n","# Now proceed with your data processing\n","print(df.columns.tolist())  # Verify the column names\n","\n","# Define the id columns based on your data structure\n","id_columns = ['MRNO', 'PM.PREFIX||\\'\\'||P.PATIENTNAME', 'AGE', 'LOOKUPVALUE', 'VISITID', 'PATIENT_VISIT_ID', 'ADMISSIONNUMBER']\n","\n","# Group by the identifier columns and aggregate the rest\n","df_grouped = df.groupby(id_columns, as_index=False).agg(lambda x: ' | '.join(x.dropna().astype(str)))\n","\n","# Save the result to a new CSV file\n","df_grouped.to_csv('output_fixed_data.csv', index=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fnl4fAaYGSIe","executionInfo":{"status":"ok","timestamp":1744129507312,"user_tz":-330,"elapsed":85251,"user":{"displayName":"Pranjal Agrawal","userId":"10165235319896773094"}},"outputId":"c47cea96-c135-4eec-e384-6202242f9cc9"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["['MRNO', \"PM.PREFIX||''||P.PATIENTNAME\", 'AGE', 'LOOKUPVALUE', 'VISITID', 'PATIENT_VISIT_ID', 'ADMISSIONNUMBER', 'LAB_ORDER_ID', 'GROUPID', 'BILLING_GROUP', 'GROUPNAME', 'DEPARTMENT_ID', 'DEPARTMENT_CODE', 'DEPARTMENT_NAME', 'EMPLOYEE_ID', 'EMPNO', \"EMPPM.PREFIX||''||EMP.FIRSTNAME||''||EMP.MIDDLENAME||''||EMP.LASTNAME\", 'PROFILE_CODE', 'PROFILE_NAME', 'SERVICE_MASTER_ID', 'SERVICE_CODE', 'SERVICE_NAME', 'INVESTIGATIONPARAMETERNAME', \"LR.RESULT_VALUE||(CASEWHENDBMS_LOB.GETLENGTH(LRP.RESULTVALUE)>3000THEN'/***********DATATRUNCATED***********/'||TO_CHAR(DBMS_LOB.SUBSTR(LRP.RESULTVALUE,3000,1))||'/***********DATATRUNCATED***********/'ELSETO_CHAR(LRP.RESULTVALUE)END)||(CASEWHENLR.RESUL\", '(CASEWHENLR.RESULT_TYPE_ID=1099THENTO_CHAR(LRP.RESULTRANGE)ELSELR.REFERENCE_RANGEEND)', \"TO_CHAR(SO.PROCESSED_DATE_TIME,'YYYY/MM/DD')\", \"TO_CHAR(LO.ORDERED_DATE,'YYYY/MM/DD')\"]\n"]}]},{"cell_type":"markdown","source":["**Step 4**"],"metadata":{"id":"KEYcwLhTGSav"}},{"cell_type":"code","source":["# Remove all the non-useful cols\n","\n","import pandas as pd\n","\n","# Load the CSV file\n","# Replace 'input_file.csv' with the actual path to your CSV file\n","file_path = 'output_fixed_data.csv'  # Update this with your file name if needed\n","df = pd.read_csv(file_path)\n","\n","# Define the columns to remove\n","columns_to_remove = [\n","    \"PM.PREFIX||''||P.PATIENTNAME\", \"VISITID\", \"PATIENT_VISIT_ID\", \"ADMISSIONNUMBER\",\n","    \"LAB_ORDER_ID\", \"GROUPID\", \"BILLING_GROUP\", \"GROUPNAME\", \"DEPARTMENT_ID\",\n","    \"DEPARTMENT_CODE\", \"DEPARTMENT_NAME\", \"EMPLOYEE_ID\", \"EMPNO\",\n","    \"EMPPM.PREFIX||''||EMP.FIRSTNAME||''||EMP.MIDDLENAME||''||EMP.LASTNAME\",\n","    \"PROFILE_CODE\", \"PROFILE_NAME\", \"SERVICE_MASTER_ID\", \"SERVICE_CODE\", \"SERVICE_NAME\", \"(CASEWHENLR.RESULT_TYPE_ID=1099THENTO_CHAR(LRP.RESULTRANGE)ELSELR.REFERENCE_RANGEEND)\",\n","    \"TO_CHAR(SO.PROCESSED_DATE_TIME,'YYYY/MM/DD')\",\n","    \"TO_CHAR(LO.ORDERED_DATE,'YYYY/MM/DD')\"\n","]\n","\n","# Remove the specified columns\n","df_cleaned = df.drop(columns=columns_to_remove, errors='ignore')\n","\n","# Save the cleaned dataframe to a new CSV file\n","output_file_path = 'filtered_data.csv'\n","df_cleaned.to_csv(output_file_path, index=False)\n","\n","print(f\"Cleaned data saved to {output_file_path}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MRnM6rLqGUv7","executionInfo":{"status":"ok","timestamp":1744129697278,"user_tz":-330,"elapsed":524,"user":{"displayName":"Pranjal Agrawal","userId":"10165235319896773094"}},"outputId":"2545ed77-3ca2-4d0a-a71e-f0e2e146c018"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Cleaned data saved to filtered_data.csv\n"]}]},{"cell_type":"markdown","source":["**Step 5**"],"metadata":{"id":"OhqCVodYGVH5"}},{"cell_type":"code","source":["# Make each Parameter an individual column and fill the corresponding value for each row in that column\n","\n","import pandas as pd\n","\n","# Load the CSV file\n","file_path = 'filtered_data.csv'  # Replace with your actual file path\n","df = pd.read_csv(file_path)\n","\n","# Specify the columns to work with\n","parameters_column = \"INVESTIGATIONPARAMETERNAME\"\n","values_column = \"LR.RESULT_VALUE||(CASEWHENDBMS_LOB.GETLENGTH(LRP.RESULTVALUE)>3000THEN'/***********DATATRUNCATED***********/'||TO_CHAR(DBMS_LOB.SUBSTR(LRP.RESULTVALUE,3000,1))||'/***********DATATRUNCATED***********/'ELSETO_CHAR(LRP.RESULTVALUE)END)||(CASEWHENLR.RESUL\"\n","\n","# Ensure all values in the columns are strings or lists; replace non-iterables with empty lists\n","df[parameters_column] = df[parameters_column].apply(lambda x: str(x).split(\" | \") if isinstance(x, str) else [])\n","df[values_column] = df[values_column].apply(lambda x: str(x).split(\" | \") if isinstance(x, str) else [])\n","\n","# Create a dictionary to hold parameter-value pairs for each row\n","expanded_data = []\n","\n","for _, row in df.iterrows():\n","    param_values = dict(zip(row[parameters_column], row[values_column]))\n","    expanded_data.append(param_values)\n","\n","# Convert the expanded data into a DataFrame\n","expanded_df = pd.DataFrame(expanded_data)\n","\n","# Concatenate the original DataFrame with the expanded DataFrame\n","result_df = pd.concat([df.drop(columns=[parameters_column, values_column]), expanded_df], axis=1)\n","\n","# Save the transformed DataFrame to a new CSV file\n","output_file_path = 'transformed_data.csv'\n","result_df.to_csv(output_file_path, index=False)\n","\n","print(f\"Transformed data saved to {output_file_path}\")\n","\n",""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Nkeo19qKGXE5","executionInfo":{"status":"ok","timestamp":1744129707531,"user_tz":-330,"elapsed":773,"user":{"displayName":"Pranjal Agrawal","userId":"10165235319896773094"}},"outputId":"cd97c34a-5e33-4c48-c102-da24188ec5f3"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Transformed data saved to transformed_data.csv\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"2lbmIgBOO0nC"},"execution_count":null,"outputs":[]}]}